---
title: "Estatística I"
subtitle: "VA Conjuntas"
author: "Rafael Bressan"
date: "Esag </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "../../css/scpo.css", "../../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: ["../../libs/partials/header.html"]
---

layout: true

<div class="my-footer"><img src="../../img/logo/UdescEsag.jpeg" style="height: 60px;"/></div> 

---
name: indice

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
    echo = FALSE,
    message = FALSE,
    warning = FALSE,
    dev = "svg",
    cache = TRUE,
    fig.align = "center"
    #fig.width = 11,
    #fig.height = 5
)

library(tidyr)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(kableExtra)
```


# Índice

- [Distribuição Conjunta](#dist-conjunta)

- [Distribuição Marginal](#dist-marginal)

- [Distribuição Condicional](#dist-condicional)

- [Independência](#independencia)

- [Exercícios 1](#exercicios1)

- [Valor Esperado Condicional](#ve-condicional)

- [Covariância e Correlação](#covariancia)

- [Funções de VA Conjuntas](#funcoes)

- [Exercícios 2](#exercicios2)

---
name: dist-conjunta

# Distribuição Conjunjta

- Interesse no comportamento conjunto de várias variáveis.

- O principal objetivo é explorar relações (similaridades) entre as variáveis

- Determinar se existe associação entre as variáveis.

--

- existe associação se existe uma mudança no comportamento de uma variável na presença de outra
  + Frequência esperada de pessoas com mais de 170 cm de altura
  + Frequência esperada de pessoas com mais de 170 cm de altura por gênero

- Se a resposta for a mesma, dizemos que não há associação

---

# Distribuição Conjunjta

Uma amostra de 20 alunos do primeiro ano de uma faculdade foi escolhida. Perguntou-se aos alunos se trabalhavam, variável que foi representada por X, e o número de vestibulares prestados, variável representada por Y . Os dados obtidos estão na tabela abaixo.

```{r exemp-conjunta-tbl,results='asis'}
df <- data.frame(
    x = rep(c("SIM", "NAO"), 3),
    y = rep(c(1, 2, 3), each = 2),
    freq = c(5, 6, 3, 4, 8, 8)
)

nobs <- sum(df$freq)

kbl(df, col.names = c("X", "Y", "Frequência")) |> 
    kable_classic(full_width = FALSE)
```

---

# Tabela de Contingência

- Distribuições bi-dimensionais são melhor descritas através de uma ***tabela de contingência*** (ou de dupla entrada) 

.pull-left[
```{r exemp-contingencia, results='asis'}
dupla_df <- pivot_wider(df, id_cols = x, names_from = y, values_from = freq)

kbl(dupla_df, col.names = c("X", 1, 2, 3),
    caption = "Frequência") |> 
    kable_classic(full_width = FALSE) |> 
    add_header_above(c(" " = 1, "Y" = 3))

```
]

--

.pull-right[
```{r exemp-prob-cont, results='asis'}
dupla_prob_df <- dupla_df |> 
    mutate(across(2:4, function(x) {x / nobs}))

freq_rel_kbl <- kbl(dupla_prob_df, col.names = c("X", 1, 2, 3),
                    caption = "Frequências Relativas") |> 
    kable_classic(full_width = FALSE) |> 
    add_header_above(c(" " = 1, "Y" = 3))
freq_rel_kbl
```
]

---

# Função de Probabilidade Conjunta

Sejam X e Y duas VAs discretas originárias do mesmo fenômeno aleatório, com valores atribuídos a partir do mesmo espaço amostral.

A ***função de probabilidade conjunta*** é definida, para todos os possíveis pares de valores $(X, Y)$, da seguinte forma:

$p(x, y) = P[(X = x) \cap (Y = y )] = P(X = x, Y = y)$

--

Ou seja, $p(x,y)$ representa a probabilidade de $X=x$ **e** $Y=y$ ao mesmo tempo (i.e. conjuntamente)

--

- Uma entrada de uma tabela de contingência representa $p(x,y)$

```{r cont-tbl, results='asis'}
freq_rel_kbl |> 
    column_spec(2, background = c("darkgreen", "white"))
```

$$P(X=SIM, Y=1)$$

---
name: dist-marginal

# Distribuição Marginal

Da função de probabilidade conjunta $p(x, y)$, é possível então obter as funções de ***probabilidade marginais*** de X e Y , através da soma sobre a outra variável:

$$P(X=x)=\sum_y p(x,y)$$

--

- As probabilidades marginais definem a distribuição da variável aleatória

- Por exemplo, dado $p(x,y)$ encontramos $P(X=x)=p(x)$. Esta é a distribuição de X

---

# Distribuição Marginal

Do exemplo anterior temos a distribuição conjunta

```{r exemp-marg, results='asis'}
dupla_prob_df <- dupla_prob_df |> 
    mutate(px = rowSums(dupla_prob_df[, 2:4]))

total_df <- dupla_prob_df |> 
    select(where(is.numeric)) |> 
    summarise(across(everything(), sum)) |> 
    mutate(x = "Py", .before = 1)

dupla_prob_df <- bind_rows(dupla_prob_df, total_df)

kbl(dupla_prob_df, col.names = c("X", 1, 2, 3, "Px"),
    caption = "Probabilidaes Marginais") |> 
    kable_classic(full_width = FALSE) |> 
    add_header_above(c(" " = 1, "Y" = 4)) |> 
    column_spec(5, background = "lightgrey") |> 
    row_spec(3, background = "lightgrey")

```

---
name: dist-condicional

# Distribuição Condicional

A probabilidade condicional de $X = x$, **dado que** $Y = y$ ocorreu, é dada pela expressão

$P(X=x\mid Y=y)=\frac{p(x,y)}{p(y)}$, se $P(Y=y)> 0$.

--

- Podemos então reescrever a probabilidade conjunta

$$P(X=x,Y=y)=P(X=x\mid Y=y)\cdot P(Y=y)$$

conhecida como a regra do produto

---
name: independencia
# Independência

- Duas va. discretas são ***independentes***, se a ocorrência de qualquer valor de uma delas não altera a probabilidade de valores da outra. Em termos matemáticos

$$P(X=x\mid Y=y)=P(X=x)$$

--

- Independência entre as variáveis aleatórias implica que a probabilidade conjunta é o produto das probabilidades marginais

$$P(X=x,Y=y)=P(X=x)\cdot P(Y=y), \quad\forall(x,y).$$

---
name: exercicios1

# Exercícios

1. Lançam-se, simultaneamente, uma moeda e um dado.

(a) Determine o espaço amostral correspondente a esse experimento.

(b) Obtenha a tabela da distribuição conjunta, considerando X o número de caras no lançamento da moeda e Y o número da face do dado.

(c) Calcule:

1. $P(X = 1)$
2. $P(X \leq 1)$
3. $P(X < 1)$
4. $P(X = 2, Y = 3)$
5. $P(X \geq 0, Y \leq 4)$
6. $P(X = 0, Y \geq 1)$

---
# Exercícios

## Solução

a) $\Omega=\{(x,y): x\in\{C,K\}, y\in\{1,\ldots , 6\}\}$

--

b) tabela da distribuição conjunta

| X\\Y | 1    | 2    | 3    | 4    | 5    | 6    | p(x) |
|------|------|------|------|------|------|------|------|
| 0    | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 |      |
| 1    | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 | 1/12 |      |
| p(y) |      |      |      |      |      |      |      |

---

# Exercícios

## Solução

c) 

1. $P(X = 1) = \sum_y p(1, y) = 6\cdot 1/12=1/2$
--

2. $P(X \leq 1) = \sum_y p(0, y)+p(1,y)= 1$

--

3. $P(X < 1) = \sum_y p(0, y) = 1/2$

--

4. $P(X = 2, Y = 3) = p(2, 3) = 0$

--

5. $P(X \geq 0, Y \leq 4) = \sum_x \sum_{y\leq 4} p(x, y)= 8\cdot 1/12 = 2/3$

--

6. $P(X = 0, Y \geq 1) = \sum_{x=0}\sum_{y\geq 1} p(x, y) = 6\cdot 1/12 = 1/2$


---

# Exercícios

2 A tabela abaixo dá a distribuição conjunta de X e Y.

(a) Determine as distribuições marginais de X e Y.

(b) Verifique se X e Y são independentes.

(c) Calcule $P(X \leq 2)$ e $P(X = 2, Y \leq 1)$.

--

| Y\\X |   1 |   2 |   3 |
|------|----:|----:|----:|
| 0    | 0,1 | 0,1 | 0,1 |
| 1    | 0,2 |   0 | 0,3 |
| 2    |   0 | 0,1 | 0,1 |


---

# Exercícios

## Solução

a) Marginais

.pull-left[
| Y\\X |   1 |   2 |   3 | p(y) |
|------|----:|----:|----:|------|
| 0    | 0,1 | 0,1 | 0,1 | 0,3  |
| 1    | 0,2 |   0 | 0,3 | 0,5  |
| 2    |   0 | 0,1 | 0,1 | 0,2  |
| p(x) | 0,3 | 0,2 | 0,5 |  1   |
]

--

.pull-right[

| X | p(x) |   | Y | p(y) |
|---|-----:|   |---|-----:|
| 1 | 0,3  |   | 0 | 0,3  |
| 2 | 0,2  |   | 1 | 0,5  |
| 3 | 0,5  |   | 2 | 0,2  |

]

---

# Exercícios

## Solução

b) Independência

.pull-left[
| Y\\X |   1 |   2 |   3 | p(y) |
|------|----:|----:|----:|------|
| 0    | 0,1 | 0,1 | 0,1 | 0,3  |
| 1    | 0,2 |   0 | 0,3 | 0,5  |
| 2    |   0 | 0,1 | 0,1 | 0,2  |
| p(x) | 0,3 | 0,2 | 0,5 |  1   |
]

--

.pull-right[
$P(X=1, Y=0)=0,1$, porém $p_X(1)=0,3$ e $p_Y(0)=0,3$. 

Logo $P(X=1, Y=0)\neq p_X(1)p_Y(0)$.

As distribuições **não** são independentes 
]

---

# Exercícios

## Solução

c) $P(X \leq 2)$ e $P(X = 2, Y \leq 1)$

$P(X \leq 2)=p_X(1)+p_X(2)=0,5$

--

$P(X = 2, Y \leq 1) = \sum_{y\leq 1} p(2, y)=0,1 + 0 = 0,1$


---
name: ve-condicional

# Valor Esperado Condicional

## Definição

Sejam $X$ e $Y$ variáveis aleatórias distribuídas conjuntamente. Denominamos de ***valor esperado condicional*** de $X$ dado $Y=y$ por:

$$E[X|Y=y]=\sum_x x P[X=x|Y=y]$$
--

- `r emo::ji("warning")` Note que $E[X|Y=y]$ é uma função de $y$! Para cada valor $Y=y$ irá corresponder um valor esperado.

--

- $E[X|Y=y]$ também é conhecido como a curva de **regressão** de $X$ em $Y$.

---

# Valor Esperado Condicional

## Exemplo

.left-thin[
| X\\Y     |   0 |   1 |   2 | $p_X(x)$ |
|----------|----:|----:|----:|---------:|
| 1        | 1/4 | 1/8 |   0 |      3/8 |
| 2        |   0 | 1/2 | 1/8 |      5/8 |
| $p_Y(y)$ | 1/4 | 5/8 | 1/8 |        1 |

]

--

.right-wide[
- Qual o $E[Y|X=1]$?

- 1º Calculamos $P[Y|X=1] = P[X=1, Y=y] / p_X(x=1)$

- $P[Y|X=1] = \{2/3; 1/3; 0\}$

- Agora, $E[Y|X=1]=\sum_y y P[Y|X=1]$

- $E[Y|X=1]0\cdot 2/3 + 1\cdot 1/3 + 2\cdot 0=1/3$
]

---

---

# Distribuições Contínuas Conjuntas

- Até o momento apresentamos apenas as definições e exemplos de va. discretas

- O caso contínuo é análogo ao discreto

--

- Somatório é substituído por uma integral

- Função de probabilidade é substituída por um função densidade de probabilidades

---

# Distribuições Contínuas Conjuntas

- Distribuição conjunta de duas variáveis é caracterizada por uma função $f(x,y)$, chamada ***função de densidade conjunta*** de X e Y, satisfazendo:

1. $f(x,y)\geq 0$
1. $\int_{-\infty}^\infty\int_{-\infty}^\infty f(x,y)\,dx\,dy = 1$
1. $P(a\leq X\leq b, c\leq Y\leq d)=\int_{a}^b\int_{c}^d f(x,y)\,dx\,dy$

--

- Densidade Marginal: $f(x)=\int_{-\infty}^\infty f(x,y)\,dy$

- Distribuição Condicional: $f(X=x\mid Y=y)=\frac{f(x,y)}{f(y)}$

- Independência: $f(x,y)=f(x)\cdot f(y)$

- Valor Esperado Condicional: $E[X\mid Y=y]=\int_x x\cdot f(X=x\mid Y=y)\,dx$

---

# Exemplo

$$
f(x,y)=
\begin{cases}
1/2, &\text{se } x\geq y, x\leq 2, x,y\geq 0\\
0, &\text{caso contrário.}
\end{cases}
$$

--

- Vejam que o suporte de $Y$ depende do valor assumido por $X$!

```{r ve-conj-cont, fig.height=3.4}
x <- seq(0, 2, by = 0.2)
y <- x
df <- data.frame(x = x, y = y)
p <- ggplot(df, aes(x, y)) +
    geom_line() +
    geom_area(fill = "darkgreen", alpha = 0.6) +
    geom_segment(aes(x = 0, y = 0, xend = 0, yend = 2.1),
                 arrow = arrow(type = "closed", length = unit(0.125, "inches")),
                 size = 0.4) +
    geom_segment(aes(x = 0, y = 0, xend = 2.1, yend = 0.0),
                 arrow = arrow(type = "closed", length = unit(0.125, "inches")),
                 size = 0.3) +

    theme_minimal()

p
```

---

# Valor Esperado Condicional

## Exemplo

- Qual o $E[Y|X]$? Ou seja, qual a curva de regressão de $Y$ em $X$?

--

$$E[Y|X] = \int_y y f(Y|X)\,dy$$

--

- Precisamos da densidade condicional $f(Y|X)$, que por sua vez depende das densidades marginais!

--

- $f(x)=\int_0^xf(x,y)\,dy = 1/2\cdot y|_0^x = x/2$

--

- $f(Y|X) = \frac{f(x,y)}{f(x)}=\frac{1/2}{x/2}=1/x$

---

# Valor Esperado Condicional

## Exemplo

- Agora podemos calcular a expressão $E[Y|X] = \int_y y\cdot f(Y|X)\,dy$

- $E[Y|X] = \int_0^x y \frac{1}{x} \,dy= \left.\frac{1}{x}\frac{y^2}{2}\right\lvert_0^x = \frac{x}{2}$

--

- Como afirmado, $E[Y|X]$ é uma função de $x$! `r emo::ji("surprise")`

---
name: covariancia

# Covariância e Correlação

- Medidas da **relação linear** entre duas variáveis aleatórias

--

$$cov(X, Y) = E[(X - E[X])(Y-E[Y])]$$

--
- O valor esperado do produto dos desvios de $X$ e $Y$ em relação às suas respectivas médias.

- Também pode ser como 

--

$$cov(X,Y)=E[XY]-E[X]E[Y]$$

--

- Quando $cov( X, Y ) = 0$, dizemos que as variáveis aleatórias $X$ e $Y$ são ***não correlacionadas***. 
---

# Covariância e Correlação

- O **coeficiente de correlação** entre $X$ e $Y$ é definido por

$$\rho(X,Y)=\frac{cov(X,Y)}{\sigma_X\sigma_Y}$$
onde $\sigma_X$ denota o desvio padrão da variável aleatória $X$

--

- Medida que não depende das unidades de $X$ e $Y$.

- $-1\leq\rho(X,Y)\leq 1$, sempre.

--

- **Propriedades**:
    + $\rho(X + a,Y + b)=\rho(X,Y)$
    + $\rho(aX,bY)=\frac{ab}{|ab|}\rho(X,Y)$

---

# Covariância e Correlação

.pull-left[
| Y\\X     |    0 |    1 |    2 | $p_Y(y)$ |
|----------|-----:|-----:|-----:|----------|
| 1        | 3/20 | 3/20 | 2/20 | 8/20     |
| 2        | 1/20 | 1/20 | 2/20 | 4/20     |
| 3        | 4/20 | 1/20 | 1/20 | 8/20     |
| $p_X(x)$ | 8/20 | 5/20 | 7/20 | 1        |

- Qual a covariância entre $X$ e $Y$?
]

--

.pull-right[
- $cov(X,Y)=E[XY]-E[X]E[Y]$

- $E[X]=\sum_{x} x\cdot p_X(x)$

- $E[Y]=\sum_{y} y\cdot p_Y(y)$

- $E[XY]=\sum_{x}\sum_{y} xy\cdot p(x,y)$
]

--

- $E[X]=0\cdot8/20 + 1\cdot 5/20 + 2\cdot 7/20 = 19/20$
- $E[Y]= ?$ 
- $E[XY]=?$

--

$$cov(X,Y)=0$$
---

# Covariância e Correlação

**Proposição**: Se $X$ e $Y$ são duas variáveis aleatórias ***independentes***, então $cov( X, Y ) = 0$.

--

$X \perp\!\!\!\!\perp Y \implies cov(X, Y)=0$

--

`r emo::ji("warning")` A recíproca **não** é verdade! Covariância nula não implica independência entre variáveis aleatórias

--

A não ser que, ... estas variáveis aleatórias sejam **Normais**!

- Se $X$ e $Y$ forem normalmente distribuídas, então $cov(X, Y)=0$ implica na independência destas variáveis.

---

# Variância

$$Var[X]=E[(X-E[X])^2]$$
- Variância é a covariância de uma va. com ela mesma!

--

**Propriedade**: $Var[X+Y]=Var[X]+Var[Y]+2\cdot cov(X,Y)$

--

$$
\begin{align*}
Var[X+Y]&=E[((X+Y)-E[X+Y])^2]\\
&=E[(X+Y)^2-2(X+Y)E[X+Y]+(E[X+Y])^2]\\
&=E[(X+Y)^2]-(E[X+Y])^2\\
&=E[X^2+2XY+Y^2]-(E[X])^2-(E[Y])^2-2E[X]E[Y]\\
&=\underbrace{E[X^2]-(E[X])^2}_{Var[X]} + \underbrace{E[Y^2]-(E[Y])^2}_{Var[Y]} + 2(\underbrace{E[XY]-E[X]E[Y]}_{cov(X,Y)})
\end{align*}
$$

---

# Variância

$$Var[X+Y]=Var[X]+Var[Y]+2\cdot cov(X,Y)$$

A ***variância da soma*** de duas variáveis aleatórias é ***igual a soma das variâncias***, se e somente se, a **covariância entre elas é igual a zero**.

--

E um Corolário desta proposição é que a independência entre duas variáveis aleatórias implica que a variância da soma é a soma das variâncias.

---
name: funcoes

# Funções de VA Conjuntas



---

name: exercicios2

# Exercícios 2

